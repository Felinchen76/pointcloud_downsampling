{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#Imports\n",
    "from io import BytesIO\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import open3d as o3d\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "cWg76JwglgE6",
    "ExecuteTime": {
     "end_time": "2024-06-29T19:43:08.077064Z",
     "start_time": "2024-06-29T19:43:04.723472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T19:43:11.196180Z",
     "start_time": "2024-06-29T19:43:10.893276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##%%\n",
    "def gromov_wasserstein(pc1: np.ndarray, pc2: np.ndarray) -> float:\n",
    "    def dist_ecc_fast(ecc, u):\n",
    "        return (np.mean(ecc <= u))\n",
    "\n",
    "    out = 0\n",
    "    # convert point clouds to numpy arrays\n",
    "    pc1 = np.asarray(pc1.points)\n",
    "    pc2 = np.asarray(pc2.points)\n",
    "\n",
    "    # Reshape input matrices if necessary\n",
    "    if pc1.ndim == 1:\n",
    "        pc1 = pc1.reshape(-1, 1)\n",
    "    if pc2.ndim == 1:\n",
    "        pc2 = pc2.reshape(-1, 1)\n",
    "\n",
    "    ecc1 = squareform(pdist(pc1)).mean(0)\n",
    "    ecc2 = squareform(pdist(pc2)).mean(0)\n",
    "    unique_ecc = np.unique(np.concatenate((ecc1, ecc2)))\n",
    "    for i in range(unique_ecc.shape[0] - 1):\n",
    "        u = unique_ecc[i]\n",
    "        out += (unique_ecc[i + 1] - unique_ecc[i]) * np.abs(dist_ecc_fast(ecc1, u) - dist_ecc_fast(ecc2, u))\n",
    "\n",
    "    return (0.5 * out)\n",
    "\n",
    "\n",
    "##%%\n",
    "def create_pcd_from_mesh(mesh):\n",
    "    mesh.compute_vertex_normals()\n",
    "    o3d.visualization.draw_geometries([mesh])\n",
    "    # distribute dots evenly on the surface\n",
    "    return mesh.sample_points_uniformly(500)\n",
    "\n",
    "\n",
    "##%%\n",
    "def load_model(link, path):\n",
    "    # http://ycb-benchmarks.s3-website-us-east-1.amazonaws.com/\n",
    "    response = requests.get(link)\n",
    "    tgz_data = BytesIO(response.content)\n",
    "    # set the current working directory to the script's directory\n",
    "    script_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "    os.chdir(script_directory)\n",
    "    with tarfile.open(fileobj=tgz_data, mode=\"r:gz\") as tar_ref:\n",
    "        tar_ref.extractall(script_directory)\n",
    "    # join paths\n",
    "    model_path = os.path.join(script_directory, path, \"clouds\", \"merged_cloud.ply\")\n",
    "    # load pointcloud\n",
    "    pcd = o3d.io.read_point_cloud(model_path)\n",
    "    return pcd\n",
    "\n",
    "\n",
    "##%%\n",
    "def load_cad_model(model):\n",
    "    # load model generated in freecad\n",
    "    return o3d.io.read_point_cloud(model)\n",
    "\n",
    "\n",
    "##%%\n",
    "def visualize_model(model):\n",
    "    o3d.visualization.draw_geometries([model])\n",
    "\n",
    "\n",
    "##%%\n",
    "def get_num_points(model):\n",
    "    print(len(model.points))\n",
    "\n",
    "\n",
    "##%%\n",
    "def create_pointcloud_from_coordinates(coordinates):\n",
    "    # create point cloud with coordinates\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(coordinates)\n",
    "    return pcd\n",
    "\n",
    "\n",
    "##%%\n",
    "def get_coordinates(model):\n",
    "    coordinates = [list(point) for point in model.points]\n",
    "    # print(coordinates[:50])\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "##%%\n",
    "def random_downsampling(model, endpoints):\n",
    "    # get coordinates of the models\n",
    "    coordinates = get_coordinates(model)\n",
    "    # select random points for downsampling\n",
    "    for i in range(len(coordinates) - endpoints):\n",
    "        rannumb = random.randint(0, len(coordinates) - 1)\n",
    "        del coordinates[rannumb]\n",
    "    point_cloud = create_pointcloud_from_coordinates(coordinates)\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "##%%\n",
    "def farthest_point_sampling(model, num_points_keep):\n",
    "    coordinates = np.array(get_coordinates(model))\n",
    "    retVal = []\n",
    "    # to make runs comparable\n",
    "    random.seed(13)\n",
    "    # generate \"random\" int\n",
    "    randint = random.randint(0, len(coordinates) - 1)\n",
    "    # select random point from model\n",
    "    retVal.append(coordinates[randint])\n",
    "    # delete chosen point from original model after it was added to the downsampled cloud\n",
    "    coordinates = np.delete(coordinates, randint, axis=0)\n",
    "    while len(retVal) < num_points_keep:\n",
    "        # Berechne die euklidischen Distanzen der ausgewÃ¤hlten Punkte zu den verbleibenden Punkten\n",
    "        eucl_distances = distance.cdist(retVal, coordinates, 'euclidean')\n",
    "        # Find point with largest min Distance\n",
    "        min_mindist = np.min(eucl_distances, axis=0)\n",
    "        # Find index of point with largest min DIstance\n",
    "        max_min_distance_index = np.argmax(min_mindist)\n",
    "        # add point that is farthest away\n",
    "        retVal.append(coordinates[max_min_distance_index])\n",
    "        # delete point from coordinates list\n",
    "        coordinates = np.delete(coordinates, max_min_distance_index, axis=0)\n",
    "    return create_pointcloud_from_coordinates(np.array(retVal))\n",
    "\n",
    "\n",
    "##%%\n",
    "# built in function von open3d\n",
    "def radius_outlier_removal_call(model):\n",
    "    return model.remove_radius_outlier(nb_points=5, radius=0.05)\n",
    "\n",
    "\n",
    "##%%\n",
    "# add noise to pointcloud\n",
    "def add_noise(model, noise_value):\n",
    "    points = np.asarray(model.points)\n",
    "    noise = np.random.normal(0, noise_value, size=points.shape)\n",
    "    noisy_points = points + noise\n",
    "\n",
    "    noisy_pc = o3d.geometry.PointCloud()\n",
    "    noisy_pc.points = o3d.utility.Vector3dVector(noisy_points)\n",
    "    return noisy_pc\n",
    "\n",
    "\n",
    "##%%\n",
    "def create_voxel_grid(model, voxel_size):\n",
    "    model_points = np.array(get_coordinates(model))\n",
    "    min_bound = np.min(model_points, axis=0)\n",
    "    max_bound = np.max(model_points, axis=0)\n",
    "\n",
    "    dimensions = np.ceil((max_bound - min_bound) / voxel_size).astype(int)\n",
    "\n",
    "    voxelgrid = np.zeros(dimensions)\n",
    "\n",
    "    for point in model_points:\n",
    "        voxel_coordinates = ((point - min_bound) / voxel_size).astype(int)\n",
    "        # -1 needed in order to avoid index out of bounds\n",
    "        voxelgrid[tuple(voxel_coordinates - 1)] += 1\n",
    "    # convert voxelgrid to open3d Voxelgrid\n",
    "    o3d_voxelgrid = o3d.geometry.VoxelGrid.create_from_point_cloud(input=model, voxel_size=voxel_size)\n",
    "    #o3d.visualization.draw_geometries([o3d_voxelgrid])\n",
    "    return o3d_voxelgrid\n",
    "\n",
    "\n",
    "def voxel_filter(model, voxelgrid, voxel_size):\n",
    "    # list where downsampled points will be saved\n",
    "    downsampled_points = []\n",
    "    # iterate over all voxel in the voxelgrid\n",
    "    for voxel in voxelgrid.get_voxels():\n",
    "        # get bounds of the voxel\n",
    "        downsampled_points.extend(is_point_in_voxel(model, voxelgrid, voxel, voxel_size))\n",
    "    downsampled_points = np.asarray(downsampled_points)\n",
    "    return create_pointcloud_from_coordinates(downsampled_points)\n",
    "\n",
    "\n",
    "def aggregate_points(points):\n",
    "    # Aggregate the points by averaging, taking into account the z coordinate\n",
    "    if len(points) == 0:\n",
    "        return points\n",
    "    aggregated_points = []\n",
    "    aggregated_points.append(np.mean(points, axis=0))\n",
    "    return aggregated_points\n",
    "\n",
    "\n",
    "def is_point_in_voxel(model, voxelgrid, voxel, voxel_size):\n",
    "    # get center point and see whether a point lies within the given distance/2 of the voxel size from the center\n",
    "    voxel_center = voxelgrid.get_voxel_center_coordinate(voxel.grid_index)\n",
    "    points_in_voxel = []\n",
    "    half_size = voxel_size / 2.0\n",
    "    # check, which points are lying within a voxel\n",
    "    for point in model.points:\n",
    "        if np.all(np.abs(point - voxel_center) <= half_size):\n",
    "            points_in_voxel.append(point)\n",
    "    points_in_voxel = aggregate_points(points_in_voxel)\n",
    "    # print(points_in_voxel)\n",
    "    return points_in_voxel\n",
    "\n",
    "\n",
    "def create_points_from_voxel(voxel_model):\n",
    "    # convert vector in numpy array\n",
    "    vector_array = np.asarray(voxel_model)\n",
    "\n",
    "    # create o3d point cloud\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(vector_array)\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "\n",
    "##%%\n",
    "def point_cloud_to_ply(point_cloud, file_name):\n",
    "    # safe downsampled point cloud as ply data\n",
    "    file_name = \"pointclouds/noise/\" + file_name + \".ply\"\n",
    "    #file_name = file_name + \".ply\"\n",
    "    #if os.path.exists(\"point_cloud_images/\" + file_name):\n",
    "    #    os.remove(file_name)\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    o3d.io.write_point_cloud(file_name, o3d.geometry.PointCloud(point_cloud.points))\n",
    "\n",
    "def point_cloud_to_ply_simple(point_cloud, file_name):\n",
    "    # safe downsampled point cloud as ply data\n",
    "    file_name = \"pointclouds/\" + file_name + \".ply\"\n",
    "    #file_name = file_name + \".ply\"\n",
    "    #if os.path.exists(\"point_cloud_images/\" + file_name):\n",
    "    #    os.remove(file_name)\n",
    "    if os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    o3d.io.write_point_cloud(file_name, o3d.geometry.PointCloud(point_cloud.points))\n",
    "\n",
    "\n",
    "##%%\n",
    "cone = load_cad_model(r\"cone.ply\")\n",
    "sphere = load_cad_model(r\"sphere.ply\")\n",
    "cube = load_cad_model(r\"cube.ply\")\n",
    "complex_cube = load_cad_model(r\"complexCube.ply\")\n",
    "complex_cone = load_cad_model(r\"hollowCone.ply\")\n",
    "complex_sphere = load_cad_model(r\"complexSphere.ply\")\n",
    "pencil = load_cad_model(r\"pencil_fein.ply\")\n",
    "# source: https://sketchfab.com/3d-models/davis-teapot-materialcleanup-547971eaf21d43f2b6cfcb6be0e7bf11\n",
    "teapot = load_cad_model(r\"teapot.ply\")\n",
    "# source: https://sketchfab.com/3d-models/book-ba04f5ac66194341bc7d437fb6c94674\n",
    "book = load_cad_model(r\"book.ply\")\n",
    "\n",
    "\n",
    "##%%\n",
    "def icp_algorithm(source, target):\n",
    "    # transform target point cloud\n",
    "    transformation = np.array([[0.86, 0.5, 0.1, 0.5],\n",
    "                               [-0.5, 0.86, 0.1, 0.99],\n",
    "                               [0.0, -0.1, 0.99, 0.5],\n",
    "                               [1.3, 0.0, 0.0, 1.0]])\n",
    "    target = target.transform(transformation)\n",
    "\n",
    "    threshold = 0.25  # max distance for deleting points\n",
    "    initial_transformation = np.identity(4)  # initial guess of transformation\n",
    "\n",
    "    # Open3D ICP Algorithmus\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, threshold, initial_transformation,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    source.transform(reg_p2p.transformation)\n",
    "    return reg_p2p\n",
    "\n",
    "\n",
    "##%%\n",
    "def write_csv(array, filename):\n",
    "    # Ãffne die CSV-Datei im Schreibmodus\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in array:\n",
    "            writer.writerow([row])\n",
    "\n",
    "##%%"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "calculation of robustness against noise"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "rd_wasserstein = []\n",
    "vf_wasserstein = []\n",
    "fp_wasserstein = []\n",
    "\n",
    "num_iterations = 25\n",
    "model_array = [cone]\n",
    "array_noise = [0.05, 0.2, 1]\n",
    "for model in model_array:\n",
    "    for noise in array_noise:\n",
    "        for i in range(num_iterations):\n",
    "            model = add_noise(model, noise)\n",
    "\n",
    "            #random downsampling\n",
    "            rd = random_downsampling(model, int(len(model.points) / 10 * 2.5))\n",
    "\n",
    "            # voxelgrid\n",
    "            vx_grid = create_voxel_grid(model, 15)\n",
    "            vx = voxel_filter(model, vx_grid, 15)\n",
    "\n",
    "            # farthest point downsampling\n",
    "            fp = farthest_point_sampling(model, int(len(model.points) / 10 * 2.5))\n",
    "\n",
    "            # Random Downsampling\n",
    "            rd_wasserstein.append(gromov_wasserstein(rd, model))\n",
    "            # Voxelgrid Filter\n",
    "            vf_wasserstein.append(gromov_wasserstein(vx, model))\n",
    "            # Farthest Point Downsampling\n",
    "            fp_wasserstein.append(gromov_wasserstein(fp, model))\n",
    "\n",
    "write_csv(rd_wasserstein, \"rd_wasserstein.csv\")\n",
    "write_csv(vf_wasserstein, \"vf_wasserstein.csv\")\n",
    "write_csv(fp_wasserstein, \"fp_wasserstein.csv\")\n",
    "\n",
    "##%%\n"
   ],
   "metadata": {
    "id": "g7lbGhU47E2x",
    "ExecuteTime": {
     "end_time": "2024-06-29T20:35:41.607206Z",
     "start_time": "2024-06-29T19:43:13.624151Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m vx \u001B[38;5;241m=\u001B[39m voxel_filter(model, vx_grid, \u001B[38;5;241m15\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# farthest point downsampling\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m fp \u001B[38;5;241m=\u001B[39m \u001B[43mfarthest_point_sampling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpoints\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2.5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Random Downsampling\u001B[39;00m\n\u001B[0;32m     24\u001B[0m rd_wasserstein\u001B[38;5;241m.\u001B[39mappend(gromov_wasserstein(rd, model))\n",
      "Cell \u001B[1;32mIn[2], line 109\u001B[0m, in \u001B[0;36mfarthest_point_sampling\u001B[1;34m(model, num_points_keep)\u001B[0m\n\u001B[0;32m    106\u001B[0m coordinates \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdelete(coordinates, randint, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(retVal) \u001B[38;5;241m<\u001B[39m num_points_keep:\n\u001B[0;32m    108\u001B[0m     \u001B[38;5;66;03m# Berechne die euklidischen Distanzen der ausgewÃ¤hlten Punkte zu den verbleibenden Punkten\u001B[39;00m\n\u001B[1;32m--> 109\u001B[0m     eucl_distances \u001B[38;5;241m=\u001B[39m \u001B[43mdistance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcdist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretVal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcoordinates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meuclidean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;66;03m# Find point with largest min Distance\u001B[39;00m\n\u001B[0;32m    111\u001B[0m     min_mindist \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmin(eucl_distances, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pointcloud_downsampling\\lib\\site-packages\\scipy\\spatial\\distance.py:2980\u001B[0m, in \u001B[0;36mcdist\u001B[1;34m(XA, XB, metric, out, **kwargs)\u001B[0m\n\u001B[0;32m   2978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m metric_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2979\u001B[0m     cdist_fn \u001B[38;5;241m=\u001B[39m metric_info\u001B[38;5;241m.\u001B[39mcdist_func\n\u001B[1;32m-> 2980\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cdist_fn(XA, XB, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2981\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mstr\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m   2982\u001B[0m     metric_info \u001B[38;5;241m=\u001B[39m _TEST_METRICS\u001B[38;5;241m.\u001B[39mget(mstr, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "time measurement for each method"
  },
  {
   "cell_type": "code",
   "source": [
    "rd_times = []  # List of lists for random downsampling times\n",
    "vf_times = []  # List of lists for voxel filter times\n",
    "fp_times = []  # List of lists for farthest point sampling times\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "# Methode Spalte, Modell Zeile ?\n",
    "# [][][]\n",
    "# [][][]\n",
    "model_array = [cone]\n",
    "for i, model in enumerate(model_array):\n",
    "    for round in range(num_iterations):\n",
    "        # random downsampling\n",
    "        start = time.time()\n",
    "        rd = random_downsampling(model, int(len(model.points) / 10 * 2.5))\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        rd_times.append(elapsed_time)\n",
    "\n",
    "        # voxelgrid filter\n",
    "        start = time.time()\n",
    "        vx_grid = create_voxel_grid(model, 15)\n",
    "        vx = voxel_filter(model, vx_grid, 15)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        vf_times.append(elapsed_time)\n",
    "\n",
    "        # farthest point downsampling\n",
    "        start = time.time()\n",
    "        fp = farthest_point_sampling(model, int(len(model.points) / 10 * 2.5))\n",
    "        end = time.time()\n",
    "        fp_pc = o3d.geometry.PointCloud()\n",
    "        fp_pc.points = o3d.utility.Vector3dVector(np.asarray(fp.points))\n",
    "        elapsed_time = end - start\n",
    "        fp_times.append(elapsed_time)  # Add the time to the corresponding model's list\n",
    "\n",
    "write_csv(rd_times, \"rd_times.csv\")\n",
    "write_csv(vf_times, \"vf_times.csv\")\n",
    "write_csv(fp_times, \"fp_times.csv\")\n",
    "##%%\n"
   ],
   "metadata": {
    "id": "-QNGchN17OJr",
    "ExecuteTime": {
     "end_time": "2024-06-29T22:11:24.996059Z",
     "start_time": "2024-06-29T22:05:24.571153Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "benchmark for icp on original pcl"
  },
  {
   "cell_type": "code",
   "source": [
    "# Vergleich auf den Originalwolken\n",
    "original_fitness = []\n",
    "original_inlier = []\n",
    "original_icp_time = []\n",
    "\n",
    "model_array = [cone]\n",
    "\n",
    "for model in model_array:\n",
    "    # ICP auf den Originalpunktewolken\n",
    "    start = time.time()\n",
    "    org_icp = icp_algorithm(model, model)\n",
    "    end = time.time()\n",
    "    original_icp_time.append(end - start)\n",
    "    original_fitness.append(org_icp.fitness)\n",
    "    original_inlier.append(org_icp.inlier_rmse)\n",
    "\n",
    "write_csv(original_inlier, \"original_icp_inlier.csv\")\n",
    "write_csv(original_fitness, \"original_icp_fitness.csv\")\n",
    "write_csv(original_icp_time, \"original_icp_time.csv\")\n",
    "\n",
    "##%%\n"
   ],
   "metadata": {
    "id": "QHjudfpt8CgS",
    "ExecuteTime": {
     "end_time": "2024-06-29T22:12:05.454091Z",
     "start_time": "2024-06-29T22:12:05.364181Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test for icp performance with downsampling"
  },
  {
   "cell_type": "code",
   "source": [
    "rd_icp_fitness = []\n",
    "rd_icp_inlier = []\n",
    "vf_icp_fitness = []\n",
    "vf_icp_inlier = []\n",
    "fp_icp_fitness = []\n",
    "fp_icp_inlier = []\n",
    "fp_icp_time = []\n",
    "vf_icp_time = []\n",
    "rd_icp_time = []\n",
    "\n",
    "model_array = [cone]\n",
    "\n",
    "for model in model_array:\n",
    "    # random downsampling\n",
    "    model_rd = random_downsampling(model, int(len(model.points) / 10 * 2.5))\n",
    "    model_rd_pc = o3d.geometry.PointCloud()\n",
    "    model_rd_pc.points = o3d.utility.Vector3dVector(np.asarray(model_rd.points))\n",
    "    start = time.time()\n",
    "    rd_icp = icp_algorithm(model_rd_pc, model)\n",
    "    end = time.time()\n",
    "    rd_icp_fitness.append(rd_icp.fitness)\n",
    "    rd_icp_inlier.append(rd_icp.inlier_rmse)\n",
    "    rd_icp_time.append(end - start)\n",
    "    #model_transformed = model.transform(rd_icp.transformation)\n",
    "\n",
    "    # show source and source and target\n",
    "    #o3d.visualization.draw_geometries([model, model_transformed])\n",
    "\n",
    "    # voxel grid filter\n",
    "    model_voxel_grid = create_voxel_grid(model, 15)\n",
    "    model_voxel = voxel_filter(model, model_voxel_grid, 15)\n",
    "    start = time.time()\n",
    "    vf_icp = icp_algorithm(model_voxel, model)\n",
    "    end = time.time()\n",
    "    vf_icp_time.append(end - start)\n",
    "    vf_icp_fitness.append(vf_icp.fitness)\n",
    "    vf_icp_inlier.append(vf_icp.inlier_rmse)\n",
    "    #model_transformed = model.transform(vf_icp.transformation)\n",
    "\n",
    "    # show source and source and target\n",
    "    #o3d.visualization.draw_geometries([model, model_transformed])\n",
    "\n",
    "    # farthest point downsampling#\n",
    "    model_fp = farthest_point_sampling(model, int(len(model.points) / 10 * 2.5))\n",
    "    model_fp_pc = o3d.geometry.PointCloud()\n",
    "    model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(model_fp.points))\n",
    "    start = time.time()\n",
    "    fp_icp = icp_algorithm(model_fp_pc, model)\n",
    "    end = time.time()\n",
    "    fp_icp_time.append(end - start)\n",
    "    fp_icp_fitness.append(fp_icp.fitness)\n",
    "    fp_icp_inlier.append(fp_icp.inlier_rmse)\n",
    "    #model_transformed = model.transform(fp_icp.transformation)\n",
    "\n",
    "    # show source and source and target\n",
    "    #o3d.visualization.draw_geometries([model, model_transformed])\n",
    "\n",
    "write_csv(rd_icp_fitness, \"rd_icp_fitness.csv\")\n",
    "write_csv(vf_icp_fitness, \"vf_icp_fitness.csv\")\n",
    "write_csv(fp_icp_fitness, \"fp_icp_fitness.csv\")\n",
    "write_csv(rd_icp_inlier, \"rd_icp_inlier.csv\")\n",
    "write_csv(vf_icp_inlier, \"vf_icp_inlier.csv\")\n",
    "write_csv(fp_icp_inlier, \"fp_icp_inlier.csv\")\n",
    "write_csv(rd_icp_time, \"rd_icp_time.csv\")\n",
    "write_csv(vf_icp_time, \"vf_icp_time.csv\")\n",
    "write_csv(fp_icp_time, \"fp_icp_time.csv\")\n",
    "##%%\n"
   ],
   "metadata": {
    "id": "Za-_Qt0t8Lfy",
    "ExecuteTime": {
     "end_time": "2024-06-29T22:13:10.980343Z",
     "start_time": "2024-06-29T22:12:31.474277Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test for accuracy with noise on pcl"
  },
  {
   "cell_type": "code",
   "source": [
    "model_array = [cube, cone, sphere]\n",
    "model_names = [\"cube\", \"cone\", \"sphere\"]\n",
    "for index, model in enumerate(model_array):\n",
    "    # create noisy pointclouds\n",
    "    noisy_model = add_noise(model, 2.5)\n",
    "\n",
    "    # Random Downsampling\n",
    "    rd_noisy = random_downsampling(noisy_model, int(len(noisy_model.points) / 10 * 2.5))\n",
    "    point_cloud_to_ply(rd_noisy, \"noisy_rd_\" + model_names[index])\n",
    "\n",
    "    # Voxel Grid Filter\n",
    "    noisy_model_grid = create_voxel_grid(noisy_model, 15)\n",
    "    noisy_model_voxel_pc = voxel_filter(noisy_model, noisy_model_grid, 15)\n",
    "    point_cloud_to_ply(noisy_model_voxel_pc, \"noisy_vf_\" + model_names[index])\n",
    "\n",
    "    # Farthest Point Downsampling\n",
    "    noisy_model_fp = farthest_point_sampling(noisy_model, int(len(noisy_model.points) / 10 * 2.5))\n",
    "    noisy_model_fp_pc = o3d.geometry.PointCloud()\n",
    "    noisy_model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(noisy_model_fp.points))\n",
    "    point_cloud_to_ply(noisy_model_fp_pc, \"noisy_fp_\" + model_names[index])"
   ],
   "metadata": {
    "id": "6WNZTiZU8P8Z",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-29T22:14:00.409565Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##%%\n",
    "model_array = [complex_sphere, complex_cube, complex_cone]\n",
    "model_names = [\"complex_sphere\", \"complex_cube\", \"complex_cone\"]\n",
    "\n",
    "for index, complex_model in enumerate(model_array):\n",
    "    noisy_complex_model = add_noise(complex_model, 4.2)\n",
    "    rd_complex_noisy = random_downsampling(noisy_complex_model, int(len(noisy_complex_model.points) / 10 * 1.5))\n",
    "    point_cloud_to_ply(rd_complex_noisy, \"noisy_rd_\" + model_names[index])\n",
    "\n",
    "    noisy_complex_model_grid = create_voxel_grid(noisy_complex_model, 10)\n",
    "    noisy_complex_model_voxel_pc = voxel_filter(noisy_complex_model, noisy_complex_model_grid, 10)\n",
    "    point_cloud_to_ply(noisy_complex_model_voxel_pc, \"noisy_vf_\" + model_names[index])\n",
    "\n",
    "    noisy_complex_model_fp = farthest_point_sampling(noisy_complex_model,\n",
    "                                                     int(len(noisy_complex_model.points) / 10 * 1.5))\n",
    "    noisy_complex_model_fp_pc = o3d.geometry.PointCloud()\n",
    "    noisy_complex_model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(noisy_complex_model_fp.points))\n",
    "    point_cloud_to_ply(noisy_complex_model_fp_pc, \"noisy_fp_\" + model_names[index])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_array = [teapot, pencil]\n",
    "model_names = [\"teapot\", \"pencil\"]\n",
    "\n",
    "for index, model_object in enumerate(model_array):\n",
    "    # random downsampling\n",
    "    noisy_model_object = add_noise(model_object, 2.5)\n",
    "    rd_noisy = random_downsampling(noisy_model_object, int(len(noisy_model_object.points) / 10 * 0.2))\n",
    "    point_cloud_to_ply(rd_noisy, \"noisy_rd_\" + model_names[index])\n",
    "\n",
    "    # voxel grid filter\n",
    "    noisy_model_object_grid = create_voxel_grid(noisy_model_object, 5)\n",
    "    noisy_object_voxel_pc = voxel_filter(noisy_model_object, noisy_model_object_grid, 5)\n",
    "    point_cloud_to_ply(noisy_object_voxel_pc, \"noisy_vf_\" + model_names[index])\n",
    "\n",
    "    # farthest point downsampling\n",
    "    noisy_model_object_fp = farthest_point_sampling(noisy_model_object, int(len(noisy_model_object.points) / 10 * 0.2))\n",
    "    noisy_model_object_fp_pc = o3d.geometry.PointCloud()\n",
    "    noisy_model_object_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(noisy_model_object_fp.points))\n",
    "    point_cloud_to_ply(noisy_model_object_fp_pc, \"noisy_fp_\" + model_names[index])\n",
    "##%%\n"
   ],
   "metadata": {
    "id": "20KQSat_Iv1i"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test for normal accuracy"
  },
  {
   "cell_type": "code",
   "source": [
    "model_array = [cube, cone, sphere]\n",
    "model_names = [\"cube\", \"cone\", \"sphere\"]\n",
    "\n",
    "for index, model in enumerate(model_array):\n",
    "    # Random Downsampling\n",
    "    rd = random_downsampling(model, int(len(model.points) / 10 * 2.5))\n",
    "    point_cloud_to_ply_simple(rd, \"rd_\" + model_names[index])\n",
    "    print(model_names[index])\n",
    "\n",
    "    # Voxel Grid Filter\n",
    "    model_grid = create_voxel_grid(model, 15)\n",
    "    model_voxel_pc = voxel_filter(model, model_grid, 15)\n",
    "    point_cloud_to_ply_simple(model_voxel_pc, \"vf_\" + model_names[index])\n",
    "\n",
    "    # Farthest Point Downsampling\n",
    "    model_fp = farthest_point_sampling(model, int(len(model.points) / 10 * 2.5))\n",
    "    model_fp_pc = o3d.geometry.PointCloud()\n",
    "    model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(model_fp.points))\n",
    "    point_cloud_to_ply_simple(model_fp_pc, \"fp_\" + model_names[index])\n",
    "\n",
    "##%%"
   ],
   "metadata": {
    "id": "9m8udcriI2Oj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vfgfmodel_array = [complex_sphere, complex_cube, complex_cone]\n",
    "model_names = [\"complex_sphere\", \"complex_cube\", \"complex_cone\"]\n",
    "\n",
    "for index, model in enumerate(model_array):\n",
    "    # Random Downsampling\n",
    "    rd = random_downsampling(model, int(len(model.points) / 10 * 1.5))\n",
    "    point_cloud_to_ply_simple(rd, \"rd_\" + model_names[index])\n",
    "\n",
    "    # Voxel Grid Filter\n",
    "    model_grid = create_voxel_grid(model, 10)\n",
    "    model_voxel_pc = voxel_filter(model, model_grid, 10)\n",
    "    point_cloud_to_ply_simple(model_voxel_pc, \"vf_\" + model_names[index])\n",
    "\n",
    "    # Farthest Point Downsampling\n",
    "    model_fp = farthest_point_sampling(model, int(len(model.points) / 10 * 1.5))\n",
    "    model_fp_pc = o3d.geometry.PointCloud()\n",
    "    model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(model_fp.points))\n",
    "    point_cloud_to_ply_simple(model_fp_pc, \"fp_\" + model_names[index])\n",
    "\n",
    "##%%"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#buch wieder hinzufÃ¼gen!\n",
    "model_array = [ teapot, pencil]\n",
    "model_names = [\"teapot\", \"pencil\"]\n",
    "\n",
    "for index, model in enumerate(model_array):\n",
    "    # Random Downsampling\n",
    "    rd = random_downsampling(model, int(len(model.points) / 10 * 0.2))\n",
    "    point_cloud_to_ply_simple(rd, \"rd_\" + model_names[index])\n",
    "\n",
    "    # Voxel Grid Filter\n",
    "    model_grid = create_voxel_grid(model, 5)\n",
    "    model_voxel_pc = voxel_filter(model, model_grid, 5)\n",
    "    point_cloud_to_ply_simple(model_voxel_pc, \"vf_\" + model_names[index])\n",
    "\n",
    "    # Farthest Point Downsampling\n",
    "    model_fp = farthest_point_sampling(model, int(len(model.points) / 10 * 0.2))\n",
    "    model_fp_pc = o3d.geometry.PointCloud()\n",
    "    model_fp_pc.points = o3d.utility.Vector3dVector(np.asarray(model_fp.points))\n",
    "    point_cloud_to_ply_simple(model_fp_pc, \"fp_\" + model_names[index])\n",
    "\n",
    "##%%"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotten der Laufzeit Tests"
   ],
   "metadata": {
    "id": "xMiYzIZGvTlG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv(r\"csvs/fp_times.csv\", header=None)\n",
    "data2 = pd.read_csv(r\"csvs/vf_times.csv\", header=None)\n",
    "data3 = pd.read_csv(r\"csvs/rd_times.csv\", header=None)\n",
    "\n",
    "data_list = [data1[0], data2[0], data3[0]]\n",
    "labels = ['Farthest Point Downsampling', 'Voxel Grid Filter', 'Random Downsampling']\n",
    "\n",
    "y_ticks = range(0, 6, 1)\n",
    "plt.yticks(y_ticks)\n",
    "\n",
    "# Erstellen des Boxplots\n",
    "plt.boxplot(data_list, patch_artist=True, showmeans=True, labels=labels)\n",
    "\n",
    "# Plot anpassen\n",
    "plt.title('Boxplot for times of downsampling methods')\n",
    "plt.xlabel('methods')\n",
    "plt.ylabel('times')\n",
    "plt.grid(True)  # Gitter anzeigen (optional)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ZRr-YQ1KuMO-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv(r\"csvs/fp_icp_time.csv\", header=None)\n",
    "data2 = pd.read_csv(r\"csvs/vf_icp_time.csv\", header=None)\n",
    "data3 = pd.read_csv(r\"csvs/rd_icp_time.csv\", header=None)\n",
    "\n",
    "data_list = [data1[0], data2[0], data3[0]]\n",
    "labels = ['Farthest Point Downsampling', 'Voxel Grid Filter', 'Random Downsampling']\n",
    "\n",
    "# Erstellen des Boxplots\n",
    "plt.boxplot(data_list, patch_artist=True, showmeans=True, labels=labels)\n",
    "\n",
    "# Plot anpassen\n",
    "plt.title('Boxplot for icp time  of downsampling methods')\n",
    "plt.xlabel('methods')\n",
    "plt.ylabel('times')\n",
    "plt.grid(True)  # Gitter anzeigen (optional)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv(r\"csvs/fp_wasserstein.csv\", header=None)\n",
    "data2 = pd.read_csv(r\"csvs/vf_wasserstein.csv\", header=None)\n",
    "data3 = pd.read_csv(r\"csvs/rd_wasserstein.csv\", header=None)\n",
    "\n",
    "data_list = [data1[0], data2[0], data3[0]]\n",
    "labels = ['Farthest Point Downsampling', 'Voxel Grid Filter', 'Random Downsampling']\n",
    "\n",
    "# Erstellen des Boxplots\n",
    "plt.boxplot(data_list, patch_artist=True, showmeans=True, labels=labels)\n",
    "\n",
    "# Plot anpassen\n",
    "plt.title('Boxplot of wasserstein distance sof downsampling methods')\n",
    "plt.xlabel('methods')\n",
    "plt.ylabel('wasserstein distance')\n",
    "plt.grid(True)  # Gitter anzeigen (optional)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv(r\"csvs/rd_icp_fitness.csv\", header=None)\n",
    "data2 = pd.read_csv(r\"csvs/vf_icp_fitness.csv\", header=None)\n",
    "data3 = pd.read_csv(r\"csvs/fp_icp_fitness.csv\", header=None)\n",
    "\n",
    "x_labels = ['Random Downsampling', 'Voxel Grid Filter', 'Farthest Point Downsampling']\n",
    "\n",
    "# Erstellen des Scatterplots\n",
    "plt.scatter(x_labels[0], data1, color='blue', label='Random Downsampling', alpha=0.6)\n",
    "plt.scatter(x_labels[1], data2, color='green', label='Voxel Grid Filter', alpha=0.6)\n",
    "plt.scatter(x_labels[2], data3, color='red', label='Farthest Point Downsampling', alpha=0.6)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv(r\"csvs/rd_icp_inlier.csv\", header=None)\n",
    "data2 = pd.read_csv(r\"csvs/vf_icp_inlier.csv\", header=None)\n",
    "data3 = pd.read_csv(r\"csvs/fp_icp_inlier.csv\", header=None)\n",
    "\n",
    "x_labels = ['Random Downsampling', 'Voxel Grid Filter', 'Farthest Point Downsampling']\n",
    "\n",
    "# Erstellen des Scatterplots\n",
    "plt.scatter(x_labels[0], data1, color='blue', label='Random Downsampling', alpha=0.6)\n",
    "plt.scatter(x_labels[1], data2, color='green', label='Voxel Grid Filter', alpha=0.6)\n",
    "plt.scatter(x_labels[2], data3, color='red', label='Farthest Point Downsampling', alpha=0.6)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_model(load_cad_model(r\"point_cloud_images/rd_cube.ply\"))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_model(load_cad_model(r\"cone.ply\"))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
